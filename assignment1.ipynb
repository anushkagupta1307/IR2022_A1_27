{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anushkagupta1307/IR2022_A1_27/blob/main/assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cYA7434aMIMN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krz3qyF9E5FP",
        "outputId": "d0cdff33-8113-4253-a2ed-1a718bc125c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Mount the data files from google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary libraries\n",
        "import pydrive\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "ps = PorterStemmer()\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import nltk\n"
      ],
      "metadata": {
        "id": "DdaJXy4DFB1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nltk library for preprocessing\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epbNPrPVFTNR",
        "outputId": "34a86fbd-4ca7-4ea4-8831-ab4bae313d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1(A) and 1(B)**"
      ],
      "metadata": {
        "id": "MewSRcBoQlBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This method takes the Data files as input, does preprocessing steps on the data and then creates a unigram inverted index\n",
        "import os\n",
        "def preprocessing_indexing():\n",
        "  word_dict=dict()\n",
        "  doc_docID_mapping=dict()\n",
        "\n",
        "  directory = '/content/drive/MyDrive/Humor,Hist,Media,Food'\n",
        "\n",
        "  count=0\n",
        "  docID=1\n",
        "  for filename in os.scandir(directory):\n",
        "      flag=0\n",
        "      if filename.is_file():\n",
        "          #print(filename.path)\n",
        "          ccfile = open(filename.path, \"r\",encoding=\"utf8\", errors=\"surrogateescape\")\n",
        "\n",
        "          doc_docID_mapping[docID]=filename.path\n",
        "\n",
        "        \n",
        "          for aline in ccfile:\n",
        "              #Preprocessing steps included- \n",
        "              # Convert to lowercase\n",
        "              aline = aline.lower()\n",
        "              # Remove numerics\n",
        "              aline = re.sub(r'\\d+','',aline)\n",
        "              # Remove whitespaces\n",
        "              aline = re.sub(r'[^\\w\\s]','', aline)\n",
        "              # Remove special characters\n",
        "              aline= re.sub('[@_!#$%^&*()<>?/\\|}{~:]','',aline)\n",
        "              aline = aline.strip()\n",
        "              #Remove stopwords\n",
        "              aline = remove_stopwords(aline)\n",
        "\n",
        "              #Convert to tokens\n",
        "              words = word_tokenize(aline)\n",
        "              \n",
        "              #Create a dictionary for the index - the dictionary has the Root word(Stemming) as key and list of docIds as the value\n",
        "              for w in words:\n",
        "                  #If root word not already present in the dictionary then create a new key\n",
        "                  if ps.stem(w) not in word_dict:\n",
        "                    list1=[docID]\n",
        "                    word_dict[ps.stem(w)]=list1\n",
        "                    break;\n",
        "                  else:\n",
        "                    #else append the docId to the already existing list of docIds for the given word\n",
        "                    if docID not in word_dict.get(ps.stem(w)):\n",
        "                      word_dict[ps.stem(w)].append(docID)\n",
        "                    \n",
        "                    break;\n",
        "\n",
        "                  print(ps.stem(w))\n",
        "          \n",
        "          ccfile.close()\n",
        "          \n",
        "          docID=docID+1\n",
        "  totalDOCS=docID-1        \n",
        "  return word_dict,doc_docID_mapping,totalDOCS"
      ],
      "metadata": {
        "id": "QiWdUAciFb_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_dict=dict()\n",
        "doc_docID_mapping=dict()\n",
        "word_dict,doc_docID_mapping,totalDOCS=preprocessing_indexing()"
      ],
      "metadata": {
        "id": "ARZlEmzVGoVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1(C)**"
      ],
      "metadata": {
        "id": "SBL4gNUFHnNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This method is used for preprocessing data\n",
        "def preprocess(word):\n",
        "      #Convert to lowercase\n",
        "      word = word.lower()\n",
        "      #Remove numerics\n",
        "      word = re.sub(r'\\d+','',word)\n",
        "      #Remove whitespaces\n",
        "      word = re.sub(r'[^\\w\\s]','', word)\n",
        "      #Remove special characters\n",
        "      word= re.sub('[@_!#$%^&*()<>?/\\|}{~:]','',word)\n",
        "      word = word.strip()\n",
        "      #remove stopwords\n",
        "      word = remove_stopwords(word)\n",
        "      \n",
        "      #Stemming\n",
        "      return ps.stem(word)"
      ],
      "metadata": {
        "id": "aJxBZ_IyHxG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**x OR y**"
      ],
      "metadata": {
        "id": "pYXO19f3H6Uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This method takes a List of DocIds to which word x belongs and a list of DocIds to which word y belongs and then returns the resultant list after OR\n",
        "# operation and the number of comparisons done to form the resultant list\n",
        "def xory(listx,listy):\n",
        "  \n",
        "  #get the size of the two lists\n",
        "  size_1 = len(listx)\n",
        "  size_2 = len(listy)\n",
        "    \n",
        "  #Create a result list\n",
        "  res = []\n",
        "  i, j = 0, 0\n",
        "  number_of_comparison=0\n",
        "  \n",
        "  #Iterate and compare\n",
        "  while i < size_1 and j < size_2:\n",
        "      #If element of list1 is lesser than list2 then add the element to result and increment pointer for list1- also increment the number of comparisons\n",
        "      if listx[i] < listy[j]:\n",
        "        if listx[i] not in res:\n",
        "          res.append(listx[i])\n",
        "        i += 1\n",
        "        number_of_comparison=number_of_comparison+1\n",
        "    \n",
        "      #else append the element of list2 to the result and increment the list2 pointer- also increment the number of comparisons\n",
        "      else:\n",
        "        if listy[j] not in res:\n",
        "          res.append(listy[j])\n",
        "        j += 1\n",
        "        number_of_comparison=number_of_comparison+1\n",
        "    \n",
        "  res = res + listx[i:] + listy[j:]\n",
        "\n",
        "  #Return the result list and the number of comparisons done\n",
        "  return res,number_of_comparison\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TZJ7KzA2H1L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**xANDy**"
      ],
      "metadata": {
        "id": "iykS0QBSIDin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This method takes a List of DocIds to which word x belongs and a list of DocIds to which word y belongs and then returns the resultant list after AND\n",
        "# operation and the number of comparisons done to form the resultant list\n",
        "def xandy(listx,listy):\n",
        "    \n",
        "    #Create a result list\n",
        "    res = []\n",
        "    i, j = 0, 0\n",
        "\n",
        "    #get the size of the two lists\n",
        "    m = len(listx)\n",
        "    n = len(listy)\n",
        "    number_of_comparison=0\n",
        "    while i < m and j < n:\n",
        "        #If the element of list1 is smaller than list2 then increment the pointer to list1 and increase the no. of comparisons\n",
        "        if listx[i] < listy[j]:\n",
        "            i += 1\n",
        "            number_of_comparison=number_of_comparison+1\n",
        "        #else increment the pointer to list2 and increment the no. of comparisons\n",
        "        elif listy[j] < listx[i]:\n",
        "            j+= 1\n",
        "            number_of_comparison=number_of_comparison+1\n",
        "        #only if the element exists in both the lists then add it to the result list and increment both pointers\n",
        "        else:\n",
        "            res.append(listy[j])\n",
        "            j += 1\n",
        "            i += 1\n",
        "    #Return the result list and the number of comparisons done\n",
        "    return res,number_of_comparison"
      ],
      "metadata": {
        "id": "e1SLHgp4H-6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**xAnd Not y**"
      ],
      "metadata": {
        "id": "AEZ9D6qfInmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This method takes a List of DocIds to which word x belongs and a list of DocIds to which word y belongs and then returns the resultant list after AND NOT\n",
        "# operation and the number of comparisons done to form the resultant list\n",
        "def xandnoty(listx,listy,totalDOCS):\n",
        "\n",
        "    #Get the list in which y does Not belong- subtract the lists to which y belongs from total docs\n",
        "    newlisty=[]\n",
        "    for i in range(1,totalDOCS+1,1):\n",
        "      if i not in listy:\n",
        "        newlisty.append(i)\n",
        "    \n",
        "    #create a result list\n",
        "    res = []\n",
        "    number_of_comparison=0\n",
        "    i, j = 0, 0\n",
        "\n",
        "    #Get the size of both the lists\n",
        "    m = len(listx)\n",
        "    n = len(newlisty)\n",
        "    while i < m and j < n:\n",
        "        #If the element of list1 is smaller than list2 then increment the pointer to list1 and increase the no. of comparisons \n",
        "        if listx[i] < newlisty[j]:\n",
        "            i += 1\n",
        "            number_of_comparison=number_of_comparison+1\n",
        "        #else increment the pointer to list2 and increment the no. of comparisons\n",
        "        elif newlisty[j] < listx[i]:\n",
        "            j+= 1\n",
        "            number_of_comparison=number_of_comparison+1\n",
        "        #only if the element exists in x and in NewList y (Not y) then add it to the result list and increment both pointers\n",
        "        else:\n",
        "            res.append(newlisty[j])\n",
        "            j += 1\n",
        "            i += 1\n",
        "            number_of_comparison=number_of_comparison+1\n",
        "    #Return the result list and the number of comparisons done\n",
        "    return res,number_of_comparison"
      ],
      "metadata": {
        "id": "sV_Jv1mOIG5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**x OR NOT y**"
      ],
      "metadata": {
        "id": "HYaBJkmSI2gD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This method takes a List of DocIds to which word x belongs and a list of DocIds to which word y belongs and then returns the resultant list after OR NOT\n",
        "# operation and the number of comparisons done to form the resultant list\n",
        "def xornoty(listx,listy,totalDOCS):\n",
        " \n",
        " #Get the list in which y does Not belong- subtract the lists to which y belongs from total docs\n",
        "  newlisty=[]\n",
        "  for i in range(1,totalDOCS+1,1):\n",
        "      if i not in listy:\n",
        "        newlisty.append(i)\n",
        "\n",
        "  #Get the size of both the lists\n",
        "  number_of_comparison=0\n",
        "  size_1 = len(listx)\n",
        "  size_2 = len(newlisty)\n",
        "   #create a result list\n",
        "  res = []\n",
        "  i, j = 0, 0\n",
        "    \n",
        "  while i < size_1 and j < size_2:\n",
        "      #If element of list1 is lesser than newlist y then add the element to result and increment pointer for list1- also increment the number of comparisons\n",
        "      if listx[i] < newlisty[j]:\n",
        "        if listx[i] not in res:\n",
        "          res.append(listx[i])\n",
        "        i += 1\n",
        "        number_of_comparison=number_of_comparison+1\n",
        "      #else append the element of newlisty to the result and increment the list2 pointer- also increment the number of comparisons\n",
        "      else:\n",
        "        if newlisty[j] not in res:\n",
        "          res.append(newlisty[j])\n",
        "        j += 1\n",
        "        number_of_comparison=number_of_comparison+1\n",
        "    \n",
        "  res = res + listx[i:] + newlisty[j:]\n",
        "  #Return the result list and the number of comparisons done\n",
        "  return res,number_of_comparison\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O54qF0cGIyn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1(C)**"
      ],
      "metadata": {
        "id": "0DwW5WHaI9pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This block takes two words as input and an operator and performs the operation on the words and gives the result list of doc ids\n",
        "x=input(\"Please enter first word ::  \")\n",
        "y=input(\"Please enter second word ::  \")\n",
        "\n",
        "#Preprocess the words\n",
        "x= preprocess(x)\n",
        "y= preprocess(y)\n",
        "#Get the list of docIds to which the words belong from the index\n",
        "listx= word_dict.get(x)\n",
        "listy=word_dict.get(y)\n",
        "\n",
        "#take the operator input \n",
        "op=input(\"Please enter the operation :: \")\n",
        "\n",
        "#If the operation is OR call the xORy method\n",
        "if op==\"OR\" or op==\"or\":\n",
        "  res,number_of_comparison=xory(listx,listy)\n",
        "\n",
        "#If the operation is AND call the xORy method\n",
        "elif op==\"AND\" or op==\"and\":\n",
        "  res,number_of_comparison=xandy(listx,listy)\n",
        "\n",
        "#If the operation is OR NOT call the xORy method\n",
        "elif op==\"ORNOT\" or op==\"ornot\" or op==\"OR NOT\" or op==\"or not\":\n",
        "  res,number_of_comparison=xornoty(listx,listy,totalDOCS)\n",
        "\n",
        "#If the operation is AND NOT call the xORy method\n",
        "elif op==\"ANDNOT\" or op==\"andnot\" or op==\"AND NOT\" or op==\"and not\":\n",
        "  res,number_of_comparison=xandnoty(listx,listy,totalDOCS)\n",
        "\n",
        "#Print the number of documents and the number of comparisons and the document names in the result\n",
        "\n",
        "print(\"Number of documents matched: \",len(res))\n",
        "print(\"No. of comparisons required: \", number_of_comparison)\n",
        "if(len(res)!=0):\n",
        "  for i in range(len(res)):\n",
        "    print(\"doc name ::  \",doc_docID_mapping[res[i]])\n",
        "else:\n",
        "  print(\"No matching Document!!!\")\n",
        "\n",
        "#Sprockets\n",
        "#Washington"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrW0ucs6I6sG",
        "outputId": "b7770ef6-5497-437c-ae35-ac2a7d65024b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter first word ::  Sprockets\n",
            "Please enter second word ::  Washington\n",
            "Please enter the operation :: or\n",
            "Number of documents matched:  23\n",
            "No. of comparisons required:  4\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/newconst.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/onetoone.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/ohandre.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/dieter.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/poll2res.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/reagan.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/socecon.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/spydust.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/lawyer.jok\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/misc.1\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/childhoo.jok\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/lawhunt.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/inlaws1.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/candy.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/sf-zine.pub\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/wkrp.epi\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/cultmov.faq\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/humor9.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/homebrew.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/jokes1.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/reconcil.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/mlverb.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/weights.hum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1(D)**"
      ],
      "metadata": {
        "id": "oC5akUT7bVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing the data\n",
        "def preprocess(word):\n",
        "      #Convert to lowercase\n",
        "      word = word.lower()\n",
        "      #Remove numerics\n",
        "      word = re.sub(r'\\d+','',word)\n",
        "      #Remove whitespaces\n",
        "      word = re.sub(r'[^\\w\\s]','', word)\n",
        "      #Remove special characters\n",
        "      word= re.sub('[@_!#$%^&*()<>?/\\|}{~:]','',word)\n",
        "      word = word.strip()\n",
        "      #Remove stopwords\n",
        "      word = remove_stopwords(word)\n",
        "      \n",
        "      #Return the root word\n",
        "      return ps.stem(word)"
      ],
      "metadata": {
        "id": "Xlmy9WnxwVsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import re\n",
        "#take number of queries from user\n",
        "total_number=int(input(\"Please enter the number of queries ::  \"))\n",
        "\n",
        "for i in range(total_number):\n",
        "  query=input(\"Please enter the query ::  \")\n",
        "  query=query.split()\n",
        "\n",
        "#Preprocessing on input query\n",
        "  list_words=[]\n",
        "  for word in query:\n",
        "        word = word.lower()\n",
        "        word = re.sub(r'\\d+','',word)\n",
        "        word = re.sub(r'[^\\w\\s]','', word)\n",
        "        word= re.sub('[@_!#$%^&*()<>?/\\|}{~:]','',word)\n",
        "        word = word.strip()\n",
        "        word = remove_stopwords(word)\n",
        "\n",
        "        if word!='':\n",
        "          list_words.append(word)\n",
        "  \n",
        "  print(list_words)\n",
        "\n",
        "  \n",
        "  #Take the lis tof operations from the user\n",
        "  list_of_operations=[]\n",
        "  for i in range(1,len(list_words),1):\n",
        "      operation=input(\"Please enter one operation.(OR/OR NOT/AND/AND NOT) ::  \")\n",
        "      list_of_operations.append(operation)\n",
        "  \n",
        "  #Get the docids for word1 from the index\n",
        "\n",
        "  if len(list_words) ==0:\n",
        "    print(\"No word in the phrase query or all stopwords in the query\")\n",
        "    sys.exit(\"No Valid Word found in the phras query\")\n",
        " \n",
        "  word1=list_words[0]\n",
        "  list_word1=word_dict.get(word1)\n",
        "  \n",
        "  #If the word doesnt exists in the index handle the error scenario\n",
        "  if list_word1 is None:\n",
        "    print(\"Cannot find the word \", word1,\"in the dictionary\")\n",
        "    sys.exit(\"Cannot find word\")\n",
        "\n",
        "  if len(list_words) ==1:\n",
        "    print(\"Only Single word in the query, hence no further operation needed\")\n",
        "    print(\"Number of Documents retrieved is ::  \",len(list_word1))\n",
        "    for i in range(len(list_word1)):\n",
        "        print(\"doc name ::  \",doc_docID_mapping[list_word1[i]])\n",
        "    sys.exit(\"Number of Document retrieved for the same is given above\")\n",
        "\n",
        "    \n",
        "  #Get the docids for word2 from the index\n",
        "  word2=list_words[1]\n",
        "  word2=preprocess(word2)\n",
        "  list_word2=word_dict.get(word2)\n",
        "\n",
        "  #If the word doesnt exists in the index handle the error scenario\n",
        "  \n",
        "  if list_word2 is None:\n",
        "    print(\"Cannot find the word \", word2,\"in the dictionary\")\n",
        "    sys.exit(\"Cannot find word\")\n",
        "    \n",
        "  \n",
        "  total_comparisons=0\n",
        "  \n",
        "  #Call the necessary method based on the operation and add the result to list of docIds and the number of comparisons\n",
        "  #Done for first two words\n",
        "\n",
        "  if(list_of_operations[0]=='OR' or list_of_operations[0]=='or'):\n",
        "    res,c1=xory(list_word1,list_word2)\n",
        "    total_comparisons=total_comparisons+c1\n",
        "\n",
        "  elif(list_of_operations[0]=='OR NOT' or list_of_operations[0]=='or not' or list_of_operations[0]=='ORNOT' or list_of_operations[0]=='ornot'):\n",
        "    res,c2=xornoty(list_word1,list_word2,totalDOCS)\n",
        "    total_comparisons=total_comparisons+c2\n",
        "\n",
        "  elif(list_of_operations[0]=='AND NOT' or list_of_operations[0]=='and not' or list_of_operations[0]=='ANDNOT' or list_of_operations[0]=='andnot'):\n",
        "    res,c3=xandnoty(list_word1,list_word2,totalDOCS)\n",
        "    total_comparisons=total_comparisons+c3\n",
        "\n",
        "  elif(list_of_operations[0]=='AND' or list_of_operations[0]=='and'):\n",
        "    res,c4=xandy(list_word1,list_word2)\n",
        "    total_comparisons=total_comparisons+c4\n",
        "\n",
        "  #Now iterate on the rest of the words\n",
        "  #The result of first two is now combined with the third word and so on..\n",
        "  for i in range(1,len(list_of_operations),1):\n",
        "\n",
        "    nextword=list_words[i+1]\n",
        "    #Preprocess the next word\n",
        "    nextword=preprocess(nextword)\n",
        "    #get the docids for next word from the index\n",
        "    list_wordnext=word_dict.get(nextword)\n",
        "    if(list_wordnext is None):\n",
        "      print(\"Cannot find the word \", word2,\"in the dictionary\")\n",
        "      sys.exit(\"Cannot find word\")\n",
        "\n",
        "    #perform the operation and add the list of docids and the number of comparisons accordingly\n",
        "\n",
        "    if(list_of_operations[0]=='OR' or list_of_operations[0]=='or'):\n",
        "      res,c5=xory(res,list_wordnext)\n",
        "      total_comparisons=total_comparisons+c5\n",
        "\n",
        "    elif(list_of_operations[0]=='OR NOT' or list_of_operations[0]=='or not' or list_of_operations[0]=='ORNOT' or list_of_operations[0]=='ornot'):\n",
        "      res,c6=xornoty(res,list_wordnext,totalDOCS)\n",
        "      total_comparisons=total_comparisons+c6\n",
        "\n",
        "    elif(list_of_operations[0]=='AND NOT' or list_of_operations[0]=='and not' or list_of_operations[0]=='ANDNOT' or list_of_operations[0]=='andnot'):\n",
        "      res,c7=xandnoty(res,list_wordnext,totalDOCS)\n",
        "      total_comparisons=total_comparisons+c7\n",
        "\n",
        "    elif(list_of_operations[0]=='AND' or list_of_operations[0]=='and'):\n",
        "      res,c8=xandy(res,list_wordnext)\n",
        "      total_comparisons=total_comparisons+c8\n",
        "\n",
        "  #Print the nummber of documents matched and the list of documents which contain the query\n",
        "\n",
        "  print(\"Number of documents matched: \",len(res))\n",
        "\n",
        "  #Print the number of comparisons required\n",
        "  print(\"No. of comparisons required: \", total_comparisons)\n",
        "  if(len(res)!=0):\n",
        "      for i in range(len(res)):\n",
        "        print(\"doc name ::  \",doc_docID_mapping[res[i]])\n",
        "  else:\n",
        "      print(\"No matching Document!!!\")\n",
        "\n",
        "  \n",
        "#Out comes a punk at dusk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Xcu0kilYKRk8",
        "outputId": "8fcfba2a-8e63-4cf4-f4be-c03c09fdf533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the number of queries ::  1\n",
            "Please enter the query ::  fuck you\n",
            "['fuck']\n",
            "Only Single word in the query, hence no further operation needed\n",
            "Number of Documents retrieved is ::   45\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/dirtword.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/insanity.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/lawsuniv.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/killself.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/looser.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/losers86.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/cars.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/reagan.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/donut.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/fuckyou2.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/terbear.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/televisi.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/xtermin8.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/word.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/lotsa.jok\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/xibovac.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/lozers\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/jokes.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/fuck!.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/pepsideg.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/eskimo.nel\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/htswfren.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/televisi.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/kloo.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/luvstory.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/bw-summe.hat\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/variety1.asc\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/quack26.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/coffee.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/jason.fun\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/sf-zine.pub\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/horflick.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/mindvox\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/msorrow\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/cybrtrsh.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/swearfrn.hum\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/practica.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/dead2.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/dead3.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/dead4.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/dead5.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/bitnet.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/girlspeak.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/suicide2.txt\n",
            "doc name ::   /content/drive/MyDrive/Humor,Hist,Media,Food/manners.txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Number of Document retrieved for the same is given above\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2(A) and Question 2(B)**"
      ],
      "metadata": {
        "id": "Hr7B6VBcfr84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This method takes the data files as input and preprocesses them and creates a Positional index - Positional Index is a dictionary of dictionary-\n",
        "#The key is the word- which has a key of DOCID and then a list of positions in respective documents as the values\n",
        "import os\n",
        "def preprocess_and_positional_index():\n",
        "  word_dict_temp=dict()\n",
        "\n",
        "  directory = '/content/drive/MyDrive/Humor,Hist,Media,Food'\n",
        "\n",
        "  count=0\n",
        "  docID=1\n",
        "  #Read the data files\n",
        "  for filename in os.scandir(directory):\n",
        "      flag=0\n",
        "      if filename.is_file():\n",
        "          #print(filename.path)\n",
        "          ccfile = open(filename.path, \"r\",encoding=\"utf8\", errors=\"surrogateescape\")\n",
        "          counter_word_position=1\n",
        "        \n",
        "          #preprocessing on the data\n",
        "          for aline in ccfile:\n",
        "              #Convert to lowercase\n",
        "              aline = aline.lower()\n",
        "              #Remove numerics\n",
        "              aline = re.sub(r'\\d+','',aline)\n",
        "              #Remove whitespaces\n",
        "              aline = re.sub(r'[^\\w\\s]','', aline)\n",
        "              #Remove special characters\n",
        "              aline= re.sub('[@_!#$%^&*()<>?/\\|}{~:]','',aline)\n",
        "              aline = aline.strip()\n",
        "              #Remove stopwords\n",
        "              aline = remove_stopwords(aline)\n",
        "              #Tokenize the words\n",
        "              words = word_tokenize(aline)\n",
        "\n",
        "\n",
        "\n",
        "              for w in words:\n",
        "                  #If the word doesnt already exist in dictionary then add it\n",
        "                  if ps.stem(w) not in word_dict_temp:\n",
        "                    dict1=dict()\n",
        "                    dict1[docID]=[counter_word_position]\n",
        "                    word_dict_temp[ps.stem(w)]=dict1\n",
        "                    counter_word_position=counter_word_position+1\n",
        "                  #If the documentId of the word doesnt exist in dictionary then add the docId and the position\n",
        "                  else:\n",
        "                    dict_temp=word_dict_temp.get(ps.stem(w))\n",
        "                    if docID not in dict_temp:\n",
        "                      word_dict_temp[ps.stem(w)][docID]=[counter_word_position]\n",
        "                    #If docid already exists just append the position of the word in the list\n",
        "                    else:\n",
        "                      word_dict_temp[ps.stem(w)][docID].append(counter_word_position)\n",
        "                    \n",
        "                    counter_word_position=counter_word_position+1\n",
        "                    \n",
        "\n",
        "                  #print(ps.stem(w))\n",
        "          \n",
        "          ccfile.close()\n",
        "          \n",
        "      docID=docID+1\n",
        "  #Return the index\n",
        "  return word_dict_temp                \n"
      ],
      "metadata": {
        "id": "qyLPeWo4gdfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_dict_temp=dict()\n",
        "word_dict_temp=preprocess_and_positional_index()"
      ],
      "metadata": {
        "id": "tL-iez2pmS_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This method finds intersection on two lists\n",
        "def intersection(lst1, lst2):\n",
        "    lst3 = [value for value in lst1 if value in lst2]\n",
        "    return lst3\n",
        "\n",
        "#Preprocessing on word\n",
        "def preprocess(word):\n",
        "      #Convert to lowercase\n",
        "      word = word.lower()\n",
        "      #Remove numerics\n",
        "      word = re.sub(r'\\d+','',word)\n",
        "      #Remove whitespaces\n",
        "      word = re.sub(r'[^\\w\\s]','', word)\n",
        "      #Remove special characters\n",
        "      word= re.sub('[@_!#$%^&*()<>?/\\|}{~:]','',word)\n",
        "      word = word.strip()\n",
        "      #Remove stop words\n",
        "      word = remove_stopwords(word)\n",
        "      #Return the Root word- stemming\n",
        "      return ps.stem(word)"
      ],
      "metadata": {
        "id": "MSOCUDM0w19j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gXRtG81ZyLrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This block takes a Phrase query from user, performs preprocessing on it and return the doc in which the phrase is present\n",
        "#It return the number of docs and the names of documents in which the phrase is present\n",
        "\n",
        "#Take the number of phrase queries as input\n",
        "total_number_phrase=int(input(\"Please enter the number of queries ::  \"))\n",
        "#word_dict_temp=dict()\n",
        "#word_dict_temp=preprocess_and_positional_index()\n",
        "\n",
        "#Take the phrase query\n",
        "for i in range(total_number_phrase):\n",
        "  query=input(\"Please enter the query ::  \")\n",
        "  word_dict_temp=dict()\n",
        "  word_dict_temp=preprocess_and_positional_index()\n",
        "  query=query.split()\n",
        "\n",
        "  #Preform preprocessing on the query\n",
        "  list_words=[]\n",
        "  for word in query:\n",
        "        #Convert to lowercase\n",
        "        word = word.lower()\n",
        "        #Remove numerics\n",
        "        word = re.sub(r'\\d+','',word)\n",
        "        #Remove whitespaces\n",
        "        word = re.sub(r'[^\\w\\s]','', word)\n",
        "        #Remove special characters\n",
        "        word= re.sub('[@_!#$%^&*()<>?/\\|}{~:]','',word)\n",
        "        word = word.strip()\n",
        "        #Remove stopwords\n",
        "        word = remove_stopwords(word)\n",
        "\n",
        "        if word!='':\n",
        "          list_words.append(word)\n",
        "\n",
        "  prerocessed_list_of_words=[]\n",
        "  for i in range(len(list_words)):\n",
        "    prerocessed_list_of_words.append(preprocess(list_words[i]))\n",
        "\n",
        "  lists=[]\n",
        "\n",
        "  for i in range(len(prerocessed_list_of_words)):\n",
        "    temp_dict=word_dict_temp.get(prerocessed_list_of_words[i])\n",
        "    lists.append(temp_dict)\n",
        "\n",
        "  result=lists[0].keys();\n",
        "\n",
        "  for i in range(1,len(prerocessed_list_of_words)):\n",
        "\n",
        "    #handle scenario where the word doesnot exist in index\n",
        "    if lists[i] is None:\n",
        "      print(\"Cannot find the word \", prerocessed_list_of_words[i],\"in the dictionary, hence cannot process further.\")\n",
        "      sys.exit(\"Cannot find the word in the dictionary\")\n",
        "    \n",
        "    result=result & lists[i].keys()\n",
        "\n",
        "  result=list(result)\n",
        "  number_of_matched_docs=0\n",
        "  flag=0\n",
        "  #Iterate on the list of words, Get the common doc ids to which they belong- Get the positions of the words in the common doc\n",
        "  for i in range(len(result)):\n",
        "    positons_in_doc=[]\n",
        "\n",
        "    #check if the next word in the phrase query is the next word in the document and so on..\n",
        "    for j in range(len(prerocessed_list_of_words)):\n",
        "      tempdict=word_dict_temp.get(prerocessed_list_of_words[j])\n",
        "      tempwordpos=tempdict.get(result[i])\n",
        "      positons_in_doc.append(tempwordpos)\n",
        "\n",
        "\n",
        "      for k in range(len(tempwordpos)):\n",
        "        tempwordpos[k]=tempwordpos[k]-j;\n",
        "\n",
        "    \n",
        "\n",
        "    #Create final position list- \n",
        "    final_positions=positons_in_doc[0]\n",
        "\n",
        "    for l in range(1,len(positons_in_doc)):\n",
        "      #print(final_positions)\n",
        "      #print(positons_in_doc[l])\n",
        "      final_positions=intersection(final_positions,positons_in_doc[l])\n",
        "      #print(final_positions)\n",
        "      #print()\n",
        "\n",
        "\n",
        "    #print(final_positions)\n",
        "\n",
        "    #Print the doc names in which the phrase is present\n",
        "    if(len(final_positions)>=1):\n",
        "      flag=1\n",
        "      #print(\"doc ID is::   \",result[i])\n",
        "      print(\"docName is::  \",doc_docID_mapping[result[i]])\n",
        "      number_of_matched_docs=number_of_matched_docs+1\n",
        "      \n",
        "\n",
        "\n",
        "  #Print the number of documents in which the phrase is present\n",
        "  print(\"Number of documents recieved :: \",number_of_matched_docs)\n",
        "  if flag==0:\n",
        "    print(\"no matching doc found\")\n",
        "\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFZ1cZXFus6k",
        "outputId": "dce6bc52-8cf3-4c4c-cf2c-bfe04673a39d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the number of queries ::  1\n",
            "Please enter the query ::  Flies can be found around window sills\n",
            "docName is::   /content/drive/MyDrive/Humor,Hist,Media,Food/bugbreak.hum\n",
            "docName is::   /content/drive/MyDrive/Humor,Hist,Media,Food/how.bugs.breakd\n",
            "docName is::   /content/drive/MyDrive/Humor,Hist,Media,Food/bugs.txt\n",
            "Number of documents recieved ::  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "o_k0sVC2y5VB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}